{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch\n",
    "from pl_bolts.models.autoencoders.components import (\n",
    "    resnet18_decoder,\n",
    "    resnet18_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEclassifier(pl.Module):\n",
    "    def __init__(self, \n",
    "            head,\n",
    "            enc_out_dim=512, \n",
    "            latent_dim=10, \n",
    "            categorical_dim=10,\n",
    "            input_height=32, \n",
    "            temperature: float = 0.5,\n",
    "            anneal_rate: float = 3e-5,\n",
    "            anneal_interval: int = 100, # every 100 batches\n",
    "            alpha: float = 30.,\n",
    "            kl_coeff = 0.1,\n",
    "            **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.l_dim = latent_dim\n",
    "        self.c_dim = categorical_dim\n",
    "\n",
    "        self.t = temperature\n",
    "        self.min_t = temperature\n",
    "        self.rate = anneal_rate\n",
    "        self.interval = anneal_interval\n",
    "        self.alpha = alpha\n",
    "        self.kl_coeff = kl_coeff\n",
    "\n",
    "        # encoder, decoder\n",
    "        self.encoder = resnet18_encoder(False, False)\n",
    "        self.decoder = resnet18_decoder(\n",
    "                        latent_dim=latent_dim * categorical_dim,\n",
    "                        input_height=input_height,\n",
    "                        first_conv=False,\n",
    "                        maxpool1=False\n",
    "                        )\n",
    "        self.head = head\n",
    "\n",
    "        # distribution parameters\n",
    "        self.fc_z = nn.Linear(enc_out_dim, latent_dim * categorical_dim)\n",
    "        \n",
    "        # for the gaussian likelihood\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def gaussian_likelihood(self, x_hat, logscale, x):\n",
    "        scale = torch.exp(logscale)\n",
    "        mean = x_hat\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "\n",
    "        # measure prob of seeing image under p(x|z)\n",
    "        log_pxz = dist.log_prob(x)\n",
    "        return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "    def kl_divergence(self, q, eps=1e-20):\n",
    "        q_p = nn.functional.softmax(q, dim=-1)\n",
    "        e = q_p * torch.log(q_p + eps)\n",
    "        ce = q_p * torch.log(1. / self.categorical_dim + eps)\n",
    "\n",
    "        kl = torch.mean(torch.sum(e - ce, dim =(1,2)), dim=0)\n",
    "        return kl\n",
    "    \n",
    "    def reparameterize(self, z, eps=1e-20):\n",
    "        u = torch.rand_like(z)\n",
    "        g = - torch.log(- torch.log(u + eps) + eps)\n",
    "\n",
    "        # Gumbel-Softmax Trick\n",
    "        s = nn.functional.softmax((z + g) / self.temp, dim=-1)\n",
    "        s = s.view(-1, self.l_dim * self.c_dim)\n",
    "        return s\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        \n",
    "        q = self.fc_z(x_encoded)\n",
    "        q = q.view(-1, self.l_dim, self.c_dim)\n",
    "        z = self.reparameterize(q)\n",
    "\n",
    "        # decoded\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        y_pred = self.head(x_hat)\n",
    "\n",
    "        if batch_idx % self.interval == 0:\n",
    "            self.t = torch.max(self.t * torch.exp(- self.rate * batch_idx),\n",
    "                                   self.min_t)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recons_loss = self.gaussian_likelihood(x_hat, self.log_scale, x) \n",
    "\n",
    "        # kl\n",
    "        kl = self.kl_divergence(q)\n",
    "\n",
    "        label_error = nn.functional.cross_entropy(y, y_pred)\n",
    "\n",
    "        # elbo\n",
    "        elbo = (self.kl_coeff)*kl - self.alpha * recons_loss\n",
    "        elbo = elbo.mean()\n",
    "\n",
    "        self.log_dict({\n",
    "            'elbo': elbo,\n",
    "            'kl': kl.mean(),\n",
    "            'recon_loss': recons_loss.mean(),\n",
    "            'cce' : label_error\n",
    "        })\n",
    "\n",
    "        return elbo + label_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

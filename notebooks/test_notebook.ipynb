{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from typing import Any, NamedTuple\n",
    "\n",
    "nn = torch.nn\n",
    "f = nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Config(NamedTuple):\n",
    "    num_heads : Any # How many decoder-classifer pairs\n",
    "    encoder : Any # Encoder function\n",
    "    decoder : Any # Decoder function\n",
    "    head : Any # Classifier function\n",
    "    input_layer : Any # Task specific input spec\n",
    "    n_class : Any # Number of Classes\n",
    "    out_dim : Any # Size of Image\n",
    "    hard : Any # argmax (T) or softmax (F)\n",
    "\n",
    "class Encoder_Config(NamedTuple):\n",
    "    n_class : Any \n",
    "    n_dist : Any # Number of categorical distributions\n",
    "    stack : Any # Internal Structure\n",
    "    dense_activation : Any # Activation function\n",
    "    tau : Any # Temperature tf variable\n",
    "\n",
    "class Decoder_Config(NamedTuple):\n",
    "    n_class : Any \n",
    "    n_dist : Any \n",
    "    stack : Any \n",
    "    dense_activation : Any\n",
    "    latent_square : Any # Size of reshaped sampled logits\n",
    "    out_dim : Any\n",
    "    tau : Any \n",
    "\n",
    "class Head_Config(NamedTuple):\n",
    "    n_class : Any\n",
    "    intermediate : Any # Task-specific layers\n",
    "    stack : Any \n",
    "    dense_activation : Any\n",
    "    in_dim : Any\n",
    "\n",
    "class Wrapper_Config(NamedTuple):\n",
    "    model : Any \n",
    "    loss : Any \n",
    "    optim : Any \n",
    "    epochs : Any \n",
    "    temp : Any \n",
    "    acc_metric : Any \n",
    "\n",
    "class Encoder_Output(NamedTuple):\n",
    "    logits_y : Any\n",
    "    p_y : Any # Fixed Prior\n",
    "\n",
    "class Decoder_Output(NamedTuple):\n",
    "    recons : Any # Reconstruced x\n",
    "    x_logits : Any \n",
    "    gen_y : Any # Generated Logits\n",
    "\n",
    "class Model_Output(NamedTuple):\n",
    "    y_pred : Any # Classifer Output\n",
    "    x_logits : Any # Reconstructed Distribition\n",
    "    gen_y : Any # Encoder Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_encoder(config):\n",
    "    def encoder():\n",
    "        layers = [] \n",
    "        in_dim = config.in_dim\n",
    "\n",
    "        for size in config.stack[:-1]:\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_dim, out_channels=size,\n",
    "                                kernel_size=config.kernel, stride=config.stride, padding=config.padding),\n",
    "                        nn.BatchNorm2d(size),\n",
    "                        nn.LeakyReLU()\n",
    "                )\n",
    "            )\n",
    "            in_dim = size\n",
    "            \n",
    "        layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_dim, out_channels=config.stack[-1],\n",
    "                                kernel_size=config.kernel, stride=config.stride, padding=config.padding)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    return encoder\n",
    "\n",
    "def init_decoder(config):\n",
    "    def decoder():\n",
    "        layers = [] \n",
    "\n",
    "        for i in range(len(config.stack) - 1):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(config.stack[i], out_channels=config.stack[i+1],\n",
    "                                kernel_size=config.kernel, stride=config.stride, padding=config.padding, output_padding=1),\n",
    "                        nn.BatchNorm2d(config.stack[i+1]),\n",
    "                        nn.LeakyReLU()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(config.stack[-2], out_channels=config.stack[-1],\n",
    "                                kernel_size=config.kernel, stride=config.stride, padding=config.padding, output_padding=1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(config.stack[-1],\n",
    "                                   config.stack[-1],\n",
    "                                   kernel_size=config.kernel,\n",
    "                                   stride=config.stride,\n",
    "                                   padding=config.padding,\n",
    "                                   output_padding=1),\n",
    "                nn.BatchNorm2d(config.stack[-1]),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(config.stack[-1], out_channels=3,\n",
    "                            kernel_size=config.kernel, padding=config.padding),\n",
    "                nn.Tanh())\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    return decoder\n",
    "\n",
    "def init_head(config):  \n",
    "    def head():\n",
    "        layers = []\n",
    "        layers.append(nn.Sequential(\n",
    "            config.base, \n",
    "            torch.flatten()\n",
    "        ))\n",
    "        for i in range(len(config.stack) - 1):\n",
    "            layers.append(nn.Sequential(\n",
    "                torch.nn.LazyLinear(config.stack[i]),\n",
    "            ))\n",
    "        layers.append(nn.Sequential(\n",
    "            torch.nn.LazyLinear(config.stack[-1]),\n",
    "            nn.softmax()\n",
    "        ))\n",
    "        return nn.Sequential(*layers)\n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialVAE(nn.Module):\n",
    "    eps = 1e-20\n",
    "    def __init__(self, config) -> None:\n",
    "        self.n_class = config.n_class\n",
    "        self.n_dist = config.n_dist\n",
    "        self.tau = config.tau\n",
    "        \n",
    "        self.encoder = config.encoder()\n",
    "        self.decoder = config.decoder()\n",
    "        self.head = config.head()\n",
    "\n",
    "        self.fc_z = nn.Linear(config.latent*4, self.n_dist * self.n_class)\n",
    "        self.scale = nn.Linear(self.n_dist * self.n_class, config.latent*4)\n",
    "\n",
    "        \n",
    "    def set_tau(self, value) -> None:\n",
    "        self.tau = value\n",
    "\n",
    "    def encode(self, input):\n",
    "        latent = self.encoder(input)\n",
    "        latent = torch.flatten(latent, start_dim=1)\n",
    "\n",
    "        z = self.fc_z(latent)\n",
    "        z = z.view(-1, self.n_dist, self.n_class)\n",
    "\n",
    "        return [z]\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.scale(z)\n",
    "        x = x.view(-1, self.latent, 2, 2)\n",
    "\n",
    "        decoded = self.decoder(x)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def reparameterize(self, z):\n",
    "        u = torch.rand_like(z)\n",
    "        g = -torch.log(-torch.log(u + self.eps) + self.eps)\n",
    "\n",
    "        logits = f.softmax((z + g) / self.tau, dim=-1)\n",
    "        return logits.view(-1, self.n_dist * self.n_class)\n",
    "\n",
    "    def sample(self):\n",
    "        return None\n",
    "\n",
    "    def generate(self):\n",
    "        return None\n",
    "\n",
    "    def forward(self, input):\n",
    "        q = self.encode(input)[0]\n",
    "        z = self.reparameterize(q)\n",
    "        recons = self.decode(z)\n",
    "        y_pred = self.predict(recons)\n",
    "        return [recons, input, q, y_pred]\n",
    "    \n",
    "    def predict(self, input):\n",
    "        return self.head(input)\n",
    "    \n",
    "    def loss(self, *args, **kwargs):\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        q = args[2]\n",
    "        y_pred = args[3]\n",
    "\n",
    "        q_p = f.softmax(q, dim=-1)\n",
    "\n",
    "        recons_loss = f.mse_loss(recons, input, reduction='mean')\n",
    "\n",
    "        h1 = q_p * torch.log(q_p + self.eps)\n",
    "        h2 = q_p * np.log(1. / self.n_dist + self.eps)\n",
    "\n",
    "        kl = q_p * np.log(1. / self.categorical_dim + self.eps)\n",
    "\n",
    "        loss = self.alpha * recons_loss +  kl\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':-kl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleVAE(nn.Module):\n",
    "    eps = 1e-20\n",
    "    def __init__(self, config) -> None:\n",
    "        self.n_class = config.n_class\n",
    "        self.n_dist = config.n_dist\n",
    "        self.n_head = config.n_head\n",
    "        self.tau = config.tau\n",
    "\n",
    "        self.encoder = config.encoder()\n",
    "        self.decoder = [config.decoder() for i in range(config.n_head)]\n",
    "        self.head = [config.head() for i in range(config.n_head)]\n",
    "\n",
    "        self.fc_z = nn.Linear(config.latent*4, self.n_dist * self.n_class)\n",
    "        self.scale = nn.Linear(self.n_dist * self.n_class, config.latent*4)\n",
    "\n",
    "        \n",
    "    def set_tau(self, value) -> None:\n",
    "        self.tau = value\n",
    "\n",
    "    def encode(self, input):\n",
    "        latent = self.encoder(input)\n",
    "        latent = torch.flatten(latent, start_dim=1)\n",
    "\n",
    "        z = self.fc_z(latent)\n",
    "        z = z.view(-1, self.n_dist, self.n_class)\n",
    "\n",
    "        return [z]\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.scale(z)\n",
    "        x = x.view(-1, self.latent, 2, 2)\n",
    "\n",
    "        decoded = self.decoder(x)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def reparameterize(self, z):\n",
    "        u = torch.rand_like(z)\n",
    "        g = -torch.log(-torch.log(u + self.eps) + self.eps)\n",
    "\n",
    "        logits = f.softmax((z + g) / self.tau, dim=-1)\n",
    "        return logits.view(-1, self.n_dist * self.n_class)\n",
    "\n",
    "    def sample(self):\n",
    "        return None\n",
    "\n",
    "    def generate(self):\n",
    "        return None\n",
    "\n",
    "    def forward(self, input):\n",
    "        q = self.encode(input)[0]\n",
    "        Z = [self.reparameterize(q) for i in range(self.n_head)]\n",
    "        X = [self.decode(z) for z in Z]\n",
    "        Y = [self.predict(x) for x in X]\n",
    "        return [X, input, q, Y]\n",
    "    \n",
    "    def predict(self, input):\n",
    "        return self.head(input)\n",
    "    \n",
    "    def loss(self, *args, **kwargs):\n",
    "        recons = args[0]\n",
    "        x = args[1]\n",
    "        q = args[2]\n",
    "        y = args[3]\n",
    "\n",
    "        q_p = f.softmax(q, dim=-1)\n",
    "\n",
    "        recons_loss = f.mse_loss(recons, input, reduction='mean')\n",
    "\n",
    "        h1 = q_p * torch.log(q_p + self.eps)\n",
    "        h2 = q_p * np.log(1. / self.n_dist + self.eps)\n",
    "\n",
    "        kl = q_p * np.log(1. / self.categorical_dim + self.eps)\n",
    "\n",
    "        loss = self.alpha * recons_loss +  kl\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':-kl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decode_config:\n",
    "    x = 1\n",
    "    y = 2\n",
    "class encode_config:\n",
    "    x = 1\n",
    "    y = 2\n",
    "class head_config:\n",
    "    x = 1\n",
    "    y = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_fn = init_encoder(encode_config)\n",
    "decoder_fn = init_encoder(decode_config)\n",
    "head_fn = init_encoder(head_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_config:\n",
    "    x = 1\n",
    "    y = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialVAE(model_config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
